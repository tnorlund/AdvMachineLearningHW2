{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Conv2D, Flatten, MaxPool2D, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from keras import models\n",
    "from keras.models import Model\n",
    "from imgaug import augmenters\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import glob, os \n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Choose a small (< 3,000) image dataset for classification. Include the link where you have downloaded the pictures from.\n",
    "\n",
    "---\n",
    "\n",
    "In order for this to work, you need a kaggle account. With this, you can download the data set here [here](https://www.kaggle.com/ivanfel/honey-bee-pollen).\n",
    "\n",
    "With this, we can read in the data at the path `/images`. This is where you must place the dataset after you download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"images/\"\n",
    "imlist= glob.glob(os.path.join(path, '*.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can read all the images, and shape them correctly. The function below reads all the images and returns the array and label for each corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(file_list,size=(300,180),flattened=False):\n",
    "    data = []\n",
    "    for i, file in enumerate(file_list):\n",
    "        image = io.imread(file)\n",
    "        image = transform.resize(image, size, mode='constant')\n",
    "        if flattened:\n",
    "            image = image.flatten()\n",
    "\n",
    "        data.append(image)\n",
    "\n",
    "    labels = [1 if f.split(\"/\")[-1][0] == 'P' else 0 for f in file_list]\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "X,Y=dataset(imlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the images loaded and labeled, we can look at the shape of the data and target of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:    (714, 300, 180, 3)\n",
      "Target:  (714,)\n"
     ]
    }
   ],
   "source": [
    "print('Data:   ',X.shape)\n",
    "print('Target: ',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that we have 714 images that are RGB. \n",
    "\n",
    "Now lets look at an example from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "k=0\n",
    "plt.sca(axes[0])\n",
    "plt.imshow(X[k])\n",
    "plt.title('Has Pollen'.format(k, Y[k]))\n",
    "\n",
    "k=400\n",
    "plt.sca(axes[1])\n",
    "plt.imshow(X[k])\n",
    "plt.title('No Pollen'.format(k, Y[k]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Goal*: Classify the bees that have pollen and those that do not.\n",
    "\n",
    "## 1\n",
    "\n",
    "Train a model from scratch using what little data you have without any regularization, to set a baseline for what can be achieved.\n",
    "\n",
    "---\n",
    "\n",
    "The first step in training the model is splitting the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=18)\n",
    "\n",
    "partial_x_train, validation_x_train, partial_y_train, validation_y_train = train_test_split(\n",
    "    x_train, y_train, test_size=0.15, random_state=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can compile a model composed of the convolution layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 300, 180, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 90, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 148, 88, 64)       36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 74, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 72, 42, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 70, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 35, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 89600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               45875712  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 46,136,385\n",
      "Trainable params: 46,136,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "verbose = 1\n",
    "# input\n",
    "input_layer = Input(shape=(300, 180, 3))\n",
    "\n",
    "# conv layers\n",
    "conv_layer1   = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "conv_layer1   = MaxPool2D( (2, 2), padding='same')(conv_layer1)\n",
    "\n",
    "conv_layer2   = Conv2D(64, (3, 3), activation='relu')(conv_layer1)\n",
    "conv_layer2   = MaxPool2D( (2, 2), padding='same')(conv_layer2)\n",
    "\n",
    "conv_layer3   = Conv2D(128, (3, 3), activation='relu')(conv_layer2)\n",
    "conv_layer3   = Conv2D(128, (3, 3), activation='relu')(conv_layer3)\n",
    "conv_layer3   = MaxPool2D( (2, 2), padding='same')(conv_layer3)\n",
    "\n",
    "conv_layer4   = Conv2D(256, (3, 3), activation='relu')(conv_layer3)\n",
    "conv_layer4   = Conv2D(256, (3, 3), activation='relu')(conv_layer4)\n",
    "conv_layer4   = MaxPool2D( (2, 2), padding='same')(conv_layer4)\n",
    "\n",
    "# flatten and dense layers\n",
    "flatten_layer = Flatten()(conv_layer3)\n",
    "dense_layer   = Dense(512, activation='relu')(flatten_layer)\n",
    "\n",
    "# output\n",
    "output_layer  = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "model_1 = Model(input_layer, output_layer)\n",
    "model_1.compile(\n",
    "    optimizer=RMSprop(lr=1e-4), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "if verbose==1:\n",
    "    print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 485 samples, validate on 86 samples\n",
      "Epoch 1/100\n",
      "435/485 [=========================>....] - ETA: 8s - loss: 0.6514 - acc: 0.6161 "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 15\n",
    "history_1 = model_1.fit(\n",
    "    partial_x_train, \n",
    "    partial_y_train,\n",
    "    validation_data=(validation_x_train, validation_y_train),\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save_weights(\"part1-1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_1.history['acc']\n",
    "val_acc = history_1.history['val_acc']\n",
    "loss = history_1.history['loss']\n",
    "val_loss = history_1.history['val_loss']\n",
    "epochs = range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r-', label='Training acc')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Training acc')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_1.evaluate(x_test, y_test, steps=10)\n",
    "print('The final test accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Use data augmentation to generate more training data from your existing training samples. Also add a Dropout layer to your model, right before the densely connected classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_layer = Input(shape=(300, 180, 3))\n",
    "\n",
    "# conv layers\n",
    "conv_layer1   = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "conv_layer1   = MaxPool2D( (2, 2), padding='same')(conv_layer1)\n",
    "\n",
    "conv_layer2   = Conv2D(64, (3, 3), activation='relu')(conv_layer1)\n",
    "conv_layer2   = MaxPool2D( (2, 2), padding='same')(conv_layer2)\n",
    "\n",
    "conv_layer3   = Conv2D(128, (3, 3), activation='relu')(conv_layer2)\n",
    "conv_layer3   = Conv2D(128, (3, 3), activation='relu')(conv_layer3)\n",
    "conv_layer3   = MaxPool2D( (2, 2), padding='same')(conv_layer3)\n",
    "\n",
    "conv_layer4   = Conv2D(256, (3, 3), activation='relu')(conv_layer3)\n",
    "conv_layer4   = Conv2D(256, (3, 3), activation='relu')(conv_layer4)\n",
    "conv_layer4   = MaxPool2D( (2, 2), padding='same')(conv_layer4)\n",
    "\n",
    "# flatten and dense layers\n",
    "flatten_layer = Flatten()(conv_layer3)\n",
    "flatten_layer = Dropout(0.5)(flatten_layer)\n",
    "dense_layer   = Dense(512, activation='relu')(flatten_layer)\n",
    "\n",
    "# output\n",
    "output_layer  = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "model_2 = Model(input_layer, output_layer)\n",
    "model_2.compile(\n",
    "    optimizer=RMSprop(lr=1e-4), \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "if verbose==1:\n",
    "    print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "datagen.fit(partial_x_train)\n",
    "history_2 = model_2.fit_generator(\n",
    "    datagen.flow(\n",
    "        partial_x_train, \n",
    "        partial_y_train, \n",
    "        batch_size=batch_size\n",
    "    ),\n",
    "    steps_per_epoch=len(partial_x_train) / batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save_weights(\"part1-2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history_2.history['acc']\n",
    "val_acc = history_2.history['val_acc']\n",
    "loss = history_2.history['loss']\n",
    "val_loss = history_2.history['val_loss']\n",
    "epochs = range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r-', label='Training acc')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Training acc')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_2.evaluate(x_test, y_test, steps=10)\n",
    "print('The final test accuracy: ',test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
